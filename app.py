import streamlit as st
import os
# Standard library imports
import traceback
from typing import TypedDict, List, Dict, Annotated, Sequence

# Third-party imports
import google.generativeai as genai
from dotenv import load_dotenv
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver # For potential state saving (optional here)


# --- Constants ---

# Node Names
PLANNER_NODE = "planner"
EXECUTOR_NODE = "executor"
REFLECTOR_NODE = "reflector"

# Default Configuration
DEFAULT_MAX_REVISIONS = 2

# Prompts - Define these outside the functions
PROMPTS = {
    "PLANNER": """You are a planning agent. Your goal is to create a concise, step-by-step plan to address the following user query.
Focus on actionable tasks. Output *only* a numbered list of tasks, one task per line.
Do NOT include any other text before or after the numbered list.

User Query: "{query}"

Plan:
1. ...
2. ...""",

    "EXECUTOR": """You are an execution agent. Perform the following task based on previous steps and provide a concise result.
Previous Executed Tasks & Results:
{executed_tasks_str}

Task to Execute: "{task}"

Concise Result:""", # Added newline after colon for clarity

    "REFLECTOR": """You are a reflection agent. Review the original query, the current plan, and the tasks executed so far.
Determine if the *remaining* plan needs modification (add, delete, modify tasks) to better achieve the original query's goal based on the execution results. Pay attention to any failed tasks or unexpected results.

Original Query: "{query}"

Full Plan:
{full_plan_str}

Executed Tasks & Results:
{executed_tasks_str}

Remaining Plan:
{remaining_plan_str}

Reflection:
1. Assess the progress: Is the plan execution going well towards the goal based on the results?
2. Identify issues: Are there problems with the remaining plan based on results? (e.g., a task is no longer needed, a new task is required, a task needs clarification, a task failed).
3. Propose Changes: If modifications are needed, output the *complete, revised plan* starting from the *next task* to be executed (the one currently at index {next_task_index_display} in the full plan). Use a numbered list format. If a task was partially completed or failed, you might adjust it or add prerequisite steps. If the current plan seems sufficient and correct, output *ONLY* the word "DONE".
4. If providing a revised plan, output *ONLY* the numbered list. Do not include any other text.

Output only the revised numbered plan (if changes are needed) or the single word "DONE".

Revised Plan or DONE:
"""
}


# --- Helper Functions ---

def format_executed_tasks(executed_tasks: List[Dict[str, str]]) -> str:
    """Formats the list of executed tasks and their results into a string."""
    if not executed_tasks:
        return "None"
    lines = []
    for task_info in executed_tasks:
        # Use .get for safety when accessing dictionary keys
        lines.append(f"- Task: {task_info.get('task', 'N/A')}\n  Result: {task_info.get('result', 'N/A')}")
    return "\n".join(lines)

def format_plan(plan: List[str], start_index: int = 0) -> str:
    """Formats a list of plan tasks into a numbered string."""
    if not plan or start_index >= len(plan):
        return "None"
    return "\n".join(f"{i + start_index + 1}. {task}" for i, task in enumerate(plan[start_index:]))

# --- LangGraph State Definition ---

class AgentState(TypedDict):
    """
    Represents the state of our LangGraph pipeline.

    Attributes:
        input_query: The original query from the user.
        plan: A list of tasks generated by the planner.
        executed_tasks: A list of dictionaries, each containing a task and its result.
        current_task_index: The index of the next task in the plan to be executed.
        reflection: The output or summary from the reflection step.
        max_revisions: The maximum number of reflection loops allowed.
        revision_number: The current count of reflection loops completed.
    """
    input_query: str
    plan: List[str]
    executed_tasks: List[Dict[str, str]]
    current_task_index: int
    reflection: str
    max_revisions: int
    revision_number: int

# --- Agent Nodes (Functions called by LangGraph) ---

@st.cache_resource # Cache the model instance
def initialize_model(api_key: str):
    """Initializes and returns the Gemini Pro model."""
    if not api_key:
         return None
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash')
        return model
    except Exception as e:
        st.error(f"Error configuring or using Gemini API. Check your API key and network connection: {e}")
        return None


def plan_step(state: AgentState, model: genai.GenerativeModel) -> Dict[str, any]:
    """
    Generates the initial plan based on the user query using the LLM.
    """
    st.write("---  Planning Step ---")
    query = state['input_query']

    try:
        response = model.generate_content(PROMPTS["PLANNER"].format(query=query))
        # Simple parsing - assumes numbered list and handles potential blank lines
        plan = [line.strip().split('. ', 1)[1] for line in response.text.strip().split('\n') if line.strip() and '. ' in line]

        if not plan:
             st.warning("Planning step did not generate a valid plan.")
             return {"plan": [], "current_task_index": 0, "revision_number": 0, "reflection": "Planning failed or produced no tasks."}


        st.write("Generated Plan:")
        st.markdown(format_plan(plan)) # Use helper function
        return {"plan": plan, "current_task_index": 0, "revision_number": 0, "reflection": ""} # Initialize reflection to empty

    except Exception as e:
        st.error(f"Error during planning: {e}")
        # Return empty plan and an error reflection on failure
        return {"plan": [], "current_task_index": 0, "revision_number": 0, "reflection": f"Planning failed: {e}"}


def execute_step(state: AgentState, model: genai.GenerativeModel) -> Dict[str, any]:
    """
    Executes the current task in the plan using the LLM as a simulated tool.
    """
    st.write("--- 🚀 Execution Step ---")
    plan = state.get('plan', [])
    task_index = state.get('current_task_index', 0)
    executed = state.get('executed_tasks', [])

    if not plan or task_index >= len(plan):
        st.warning("No more tasks to execute.")
        return {} # No state changes if no tasks left

    task = plan[task_index]
    st.write(f"Executing Task {task_index + 1}/{len(plan)}: {task}")

    # Use helper function to format executed tasks for the prompt
    executed_tasks_str = format_executed_tasks(executed)

    try:
        # Format the prompt using the defined constant and formatted string
        prompt = PROMPTS["EXECUTOR"].format(
            executed_tasks_str=executed_tasks_str,
            task=task
        )
        response = model.generate_content(prompt)
        result = response.text.strip()
        st.write(f"Task Result: {result}")

        # Append the result to executed tasks
        executed.append({"task": task, "result": result})

        # Return updated state
        return {
            "executed_tasks": executed,
            "current_task_index": task_index + 1, # Move to next task
            "reflection": "" # Clear reflection after successful execution
        }
    except Exception as e:
        st.error(f"Error during task execution (Task {task_index + 1}: '{task}'): {e}")
        # Add a failure result and add a note to reflection for the next step
        executed.append({"task": task, "result": f"Execution Failed: {e}"})
        return {
             "executed_tasks": executed,
             # current_task_index remains the same so reflection sees the failed task
             "reflection": f"Execution of task '{task}' failed: {e}"
        }


def reflect_step(state: AgentState, model: genai.GenerativeModel) -> Dict[str, any]:
    """
    Reflects on the executed tasks and potentially modifies the remaining plan using the LLM.
    Increments the revision count.
    """
    st.write("---  Reflection Step ---")
    revision_number = state.get("revision_number", 0) + 1
    max_revisions = state['max_revisions']

    if revision_number > max_revisions:
        st.warning(f"Maximum reflection revisions ({max_revisions}) reached.")
        # Set reflection status to indicate max revisions reached
        return {"revision_number": revision_number, "reflection": "Max revisions reached."}

    st.write(f"Reflection Iteration: {revision_number}/{max_revisions}")

    query = state['input_query']
    plan = state['plan']
    executed = state['executed_tasks']
    next_task_index = state['current_task_index'] # This is the index *after* the last executed task

    # Use helper functions to format strings for the prompt
    executed_tasks_str = format_executed_tasks(executed)
    remaining_plan_str = format_plan(plan, start_index=next_task_index)
    full_plan_str = format_plan(plan) # Format full plan starting from index 0

    try:
        # Format the prompt using the defined constant and formatted strings
        prompt = PROMPTS["REFLECTOR"].format(
            query=query,
            full_plan_str=full_plan_str,
            executed_tasks_str=executed_tasks_str,
            remaining_plan_str=remaining_plan_str,
            next_task_index_display=next_task_index + 1 # Display 1-based index
        )

        response = model.generate_content(prompt)
        reflection_output = response.text.strip()
        st.write(f"Reflection Output: {reflection_output}")

        # Check if the LLM indicated completion
        if reflection_output.upper() == "DONE":
            st.success("Reflection complete. Plan seems sufficient.")
            return {"reflection": "DONE", "revision_number": revision_number}
        else:
            # Attempt to parse a revised plan
            # Assumes numbered list format
            new_plan_tasks = [line.strip().split('. ', 1)[1] for line in reflection_output.split('\n') if line.strip() and '. ' in line]

            if new_plan_tasks:
                # Combine executed tasks' original plan steps with the new revised steps
                # Keep the tasks that were already executed (up to next_task_index)
                revised_full_plan = plan[:next_task_index] + new_plan_tasks
                st.write("Plan revised based on reflection:")
                st.markdown(format_plan(revised_full_plan)) # Use helper function to display revised plan

                # The current_task_index should now point to the first task in the new_plan_tasks segment,
                # which is at index next_task_index in the revised_full_plan.
                new_current_task_index = next_task_index


                return {
                    "plan": revised_full_plan,
                    "current_task_index": new_current_task_index, # Continue from the start of the revised part
                    "reflection": "Plan revised.",
                    "revision_number": revision_number
                }
            else:
                # If output is not DONE and not a valid numbered list
                st.warning("Reflection step did not output 'DONE' or a valid revised plan. Continuing with original plan.")
                return {"reflection": "No valid revision found. Continuing with existing plan.", "revision_number": revision_number}

    except Exception as e:
        st.error(f"Error during reflection: {e}")
        return {"reflection": f"Reflection failed: {e}", "revision_number": revision_number}


# --- Conditional Edges (Logic for LangGraph Flow) ---

def should_continue(state: AgentState) -> str:
    """
    Determines the next step after reflection based on the state.

    Returns:
        "continue_execution": If there are tasks remaining and conditions allow.
        "end": If reflection is DONE, max revisions are reached, or no tasks remain.
    """
    st.write("--- 🚦 Decision Point ---")
    revision_number = state.get("revision_number", 0)
    max_revisions = state['max_revisions']
    reflection_status = state.get("reflection", "").upper()
    current_task_index = state.get('current_task_index', 0)
    plan_length = len(state.get('plan', []))

    # Condition 1: Reflection explicitly says DONE - Highest priority
    if reflection_status == "DONE":
        st.write(" Decision: Reflection is 'DONE'. Ending pipeline.")
        return "end"

    # Condition 2: Maximum reflection revisions reached
    # Check this AFTER checking if reflection is DONE, but BEFORE checking if tasks remain
    if revision_number > max_revisions:
         st.write(f" Decision: Max revisions ({max_revisions}) reached. Ending pipeline.")
         return "end"

    # Condition 3: Tasks remain in the plan
    # If reflection was NOT DONE and NOT max revisions, check if more work is needed
    if current_task_index < plan_length:
        st.write(f" Decision: Tasks remain in the plan ({current_task_index + 1}/{plan_length}). Continuing to execute.")
        return "continue_execution"

    # Condition 4: No tasks left, and pipeline not explicitly finished by reflection or revisions
    # This is the fallback if none of the above conditions are met.
    st.write(" Decision: All tasks processed and pipeline not explicitly 'DONE' or max revisions hit. Ending pipeline.")
    return "end"

# --- Streamlit UI ---

def main():
    """Main function to run the Streamlit application."""
    st.set_page_config(layout="wide", page_title="LangGraph Agent Pipeline")
    st.title("LangGraph Agent Pipeline with Gemini & Streamlit")
    st.caption("Plan -> Execute -> Reflect Loop")
    st.markdown("This demo runs a simple agent loop. Enter a query, and the agent will Plan, Execute tasks, and Reflect on the results, potentially revising the plan.")

    # --- Session State Initialization ---
    # Initialize all session state keys at the very beginning
    # This prevents AttributeErrors on subsequent reruns
    if 'api_key' not in st.session_state:
        st.session_state.api_key = ""
    if 'user_query_input' not in st.session_state:
        st.session_state.user_query_input = ""
    if 'max_rev_input' not in st.session_state:
        st.session_state.max_rev_input = DEFAULT_MAX_REVISIONS
    # Add any other session state keys here if needed later

    # --- Configuration and Setup ---
    load_dotenv() # Load environment variables after session state is initialized


    # --- API Key Handling ---
    # Try to get API key from environment variable first
    env_api_key = os.environ.get("GOOGLE_API_KEY")

    # If the API key is found in environment variables, use it
    if env_api_key:
        st.session_state.api_key = env_api_key
        st.info("API Key loaded from environment variable `GOOGLE_API_KEY`.")
        # No need to display the text input if the key was found in the environment

    # If the key is NOT in environment variables or session_state is still empty,
    # display the text input.
    # The text input updates st.session_state.api_key if the user types.
    if not st.session_state.api_key:
         st.session_state.api_key = st.text_input(
             "Enter your Google Gemini API Key:",
             type="password",
             value=st.session_state.api_key, # This will be "" initially here if env_api_key is None
             key="api_key_input_widget" # Unique key for the widget
         )
         # If the user just entered it, st.session_state.api_key will now be updated

    # --- Initialize Gemini Model ---
    # Stop the app if API key is still missing (either from env or input)
    if not st.session_state.api_key:
        st.warning("Google Gemini API Key is required to run this application.")
        st.stop()

    # Now st.session_state.api_key should contain the key if we reach here
    model = initialize_model(st.session_state.api_key)

    if not model:
        # initialize_model shows an error message if it fails
        st.stop() # Stop if model initialization failed


    # --- User Input ---
    # These text inputs now rely on session state keys that are guaranteed to be initialized above
    st.session_state.user_query_input = st.text_area(
        "Enter your complex query or goal:",
        height=100,
        value=st.session_state.user_query_input, # This key is initialized at the top
        key="user_query_input_widget"
    )
    st.session_state.max_rev_input = st.number_input(
        "Maximum reflection loops:",
        min_value=0,
        max_value=10,
        value=st.session_state.max_rev_input, # This key is initialized at the top
        key="max_rev_input_widget"
    )

    # --- Build and Run LangGraph ---
    # Only build the graph when needed (e.g., when the button is clicked)
    if st.button("Run Pipeline", key="run_button"):
        if not st.session_state.user_query_input:
            st.error("Please enter a query.")
            st.stop()

        # --- Build LangGraph ---
        # Define the state graph
        workflow = StateGraph(AgentState)

        # Add nodes using defined constants
        workflow.add_node(PLANNER_NODE, lambda state: plan_step(state, model))
        workflow.add_node(EXECUTOR_NODE, lambda state: execute_step(state, model))
        workflow.add_node(REFLECTOR_NODE, lambda state: reflect_step(state, model))

        # Set entry point using constant
        workflow.set_entry_point(PLANNER_NODE)

        # Add edges using constants
        workflow.add_edge(PLANNER_NODE, EXECUTOR_NODE) # Plan -> Execute first task
        workflow.add_edge(EXECUTOR_NODE, REFLECTOR_NODE) # Execute -> Reflect

        # Conditional edge after reflection using constant
        workflow.add_conditional_edges(
            REFLECTOR_NODE,
            should_continue, # This function determines the next node
            {
                "continue_execution": EXECUTOR_NODE, # If decision is continue, go to executor
                "end": END # If decision is end, terminate
            }
        )

        # Compile the graph
        # memory = MemorySaver() # Optional: for saving state across runs
        # app = workflow.compile(checkpointer=memory)
        app = workflow.compile() # No checkpointer needed for this example

        # --- Execute the Graph ---
        st.markdown("---  Pipeline Execution Started ---")
        initial_state = AgentState(
            input_query=st.session_state.user_query_input,
            plan=[], # Plan will be filled by the planner node
            executed_tasks=[],
            current_task_index=0,
            reflection="",
            max_revisions=st.session_state.max_rev_input,
            revision_number=0
        )

        # Use containers/expanders for dynamic updates and clear previous runs
        final_result_container = st.container()
        progress_expander = st.expander("Pipeline Progress Details", expanded=True)
        progress_expander.empty() # Clear previous run's progress

        final_state = None
        try:
            with st.spinner("Pipeline running..."):
                 for current_state in app.stream(initial_state, stream_mode="values"):

                     with progress_expander:

                         st.write(f"**State Update:**")
                         st.json(current_state, expanded=False) 
                         st.markdown("---")

                     final_state = current_state 

            st.success("Pipeline finished!")

        except Exception as e:
            st.error(f"An error occurred during pipeline execution: {e}")
            st.code(traceback.format_exc())
            st.warning("Pipeline execution halted due to error.")


        with final_result_container:
            st.subheader("🏁 Final Results")
            if final_state:
                st.write("**Original Query:**", final_state.get('input_query'))
                st.write("**Final Plan:**")
                final_plan = final_state.get('plan')
                if final_plan:
                    st.markdown(format_plan(final_plan))
                else:
                    st.write("No final plan available.")

                st.write("**Executed Tasks:**")
                final_executed = final_state.get('executed_tasks')
                st.markdown(format_executed_tasks(final_executed))
            else:
                st.warning("Pipeline did not produce a final state or encountered an error early.")

    else:
        st.info("Enter your query and click 'Run Pipeline' to start the agent.")

if __name__ == "__main__":
    main()